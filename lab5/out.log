nohup: ignoring input
Epoch:0, loss=2519.296630859375
Training loss:56.50674057006836, test loss: 54.78858184814453
Epoch:1, loss=1902.8304443359375
Training loss:32.341285705566406, test loss: 32.46226501464844
Epoch:2, loss=828.2357177734375
Training loss:14.271483421325684, test loss: 15.924348831176758
Epoch:3, loss=495.4672546386719
Training loss:11.655783653259277, test loss: 13.422528266906738
Epoch:4, loss=410.8562316894531
Training loss:9.914030075073242, test loss: 11.531871795654297
Epoch:5, loss=352.16455078125
Training loss:9.4888916015625, test loss: 11.25093936920166
Epoch:6, loss=308.5771789550781
Training loss:6.714659214019775, test loss: 8.601760864257812
Epoch:7, loss=259.0440673828125
Training loss:6.006522178649902, test loss: 7.877361297607422
Epoch:8, loss=237.7077178955078
Training loss:6.089353084564209, test loss: 7.951446533203125
Epoch:9, loss=221.6004180908203
Training loss:5.273977279663086, test loss: 6.996922492980957
Epoch:10, loss=212.39132690429688
Training loss:4.893618106842041, test loss: 6.634471416473389
Epoch:11, loss=200.94903564453125
Training loss:5.131350994110107, test loss: 6.547867774963379
Epoch:12, loss=184.30886840820312
Training loss:4.307940483093262, test loss: 5.897738456726074
Epoch:13, loss=179.58395385742188
Training loss:4.403805732727051, test loss: 5.645441055297852
Epoch:14, loss=163.56837463378906
Training loss:3.831683874130249, test loss: 5.215384006500244
Epoch:15, loss=155.94076538085938
Training loss:3.7016472816467285, test loss: 4.915586471557617
Epoch:16, loss=151.996826171875
Training loss:3.605032444000244, test loss: 4.735967636108398
Epoch:17, loss=152.04681396484375
Training loss:6.2733473777771, test loss: 7.609132289886475
Epoch:18, loss=173.05584716796875
Training loss:3.298413038253784, test loss: 4.340371131896973
Epoch:19, loss=133.78025817871094
Training loss:4.151397228240967, test loss: 5.197999477386475
Epoch:20, loss=143.5415496826172
Training loss:3.1936194896698, test loss: 4.352888107299805
Epoch:21, loss=121.71533203125
Training loss:2.9096314907073975, test loss: 3.826245069503784
Epoch:22, loss=121.5538330078125
Training loss:2.8044140338897705, test loss: 3.73293399810791
Epoch:23, loss=114.30243682861328
Training loss:2.8879354000091553, test loss: 3.6389541625976562
Epoch:24, loss=118.98228454589844
Training loss:3.7697930335998535, test loss: 4.539039611816406
Epoch:25, loss=118.73298645019531
Training loss:2.69704008102417, test loss: 3.547546625137329
Epoch:26, loss=104.82645416259766
Training loss:2.515472650527954, test loss: 3.1854910850524902
Epoch:27, loss=103.6949691772461
Training loss:2.4563698768615723, test loss: 3.140512466430664
Epoch:28, loss=99.13074493408203
Training loss:2.6599583625793457, test loss: 3.1580610275268555
Epoch:29, loss=94.8797836303711
Training loss:2.28174090385437, test loss: 2.90372371673584
Epoch:30, loss=95.84685516357422
Training loss:2.1909942626953125, test loss: 2.9206089973449707
Epoch:31, loss=91.40571594238281
Training loss:3.364392042160034, test loss: 3.970241069793701
Epoch:32, loss=95.21693420410156
Training loss:2.4850893020629883, test loss: 3.0378661155700684
Epoch:33, loss=86.74362182617188
Training loss:2.356964349746704, test loss: 2.9194064140319824
Epoch:34, loss=89.78194427490234
Training loss:2.0666043758392334, test loss: 2.5817923545837402
Epoch:35, loss=84.96440124511719
Training loss:2.0000736713409424, test loss: 2.5956337451934814
Epoch:36, loss=80.69822692871094
Training loss:1.9389082193374634, test loss: 2.5656204223632812
Epoch:37, loss=83.17990112304688
Training loss:2.051090717315674, test loss: 2.5780673027038574
Epoch:38, loss=77.03136444091797
Training loss:2.1374564170837402, test loss: 2.7622599601745605
Epoch:39, loss=78.86201477050781
Training loss:1.9644157886505127, test loss: 2.4561848640441895
Epoch:40, loss=77.76184844970703
Training loss:1.7687771320343018, test loss: 2.3346753120422363
Epoch:41, loss=74.39646911621094
Training loss:1.835184931755066, test loss: 2.417891025543213
Epoch:42, loss=71.45793151855469
Training loss:1.8392105102539062, test loss: 2.370971918106079
Epoch:43, loss=73.26932525634766
Training loss:1.9996941089630127, test loss: 2.6522252559661865
Epoch:44, loss=72.39010620117188
Training loss:1.6803807020187378, test loss: 2.201167106628418
Epoch:45, loss=69.74613952636719
Training loss:1.9980573654174805, test loss: 2.381453275680542
Epoch:46, loss=67.21416473388672
Training loss:1.569480299949646, test loss: 2.0633342266082764
Epoch:47, loss=67.63040924072266
Training loss:2.7190804481506348, test loss: 3.214876890182495
Epoch:48, loss=76.03533172607422
Training loss:1.7975612878799438, test loss: 2.3161680698394775
Epoch:49, loss=69.14661407470703
Training loss:1.5778696537017822, test loss: 2.062439441680908
Epoch:50, loss=63.78919982910156
Training loss:1.468034029006958, test loss: 1.9826796054840088
Epoch:51, loss=66.39794921875
Training loss:1.7246042490005493, test loss: 2.2030837535858154
Epoch:52, loss=68.62918853759766
Training loss:1.6482971906661987, test loss: 2.0462327003479004
Epoch:53, loss=60.031951904296875
Training loss:2.2139813899993896, test loss: 2.524517297744751
Epoch:54, loss=70.61205291748047
Training loss:1.4972453117370605, test loss: 1.9674251079559326
Epoch:55, loss=64.81279754638672
Training loss:1.438685655593872, test loss: 1.8303951025009155
Epoch:56, loss=60.355979919433594
Training loss:1.8039560317993164, test loss: 2.2932090759277344
Epoch:57, loss=60.56226348876953
Training loss:1.5218552350997925, test loss: 1.9893853664398193
Epoch:58, loss=55.36155700683594
Training loss:1.4773662090301514, test loss: 1.975875735282898
Epoch:59, loss=60.75048828125
Training loss:2.277301788330078, test loss: 2.7213828563690186
Epoch:60, loss=57.49675750732422
Training loss:1.3490424156188965, test loss: 1.6921677589416504
Epoch:61, loss=54.105926513671875
Training loss:1.3100301027297974, test loss: 1.6997642517089844
Epoch:62, loss=52.703765869140625
Training loss:1.3274699449539185, test loss: 1.7706698179244995
Epoch:63, loss=55.30897903442383
Training loss:1.3210124969482422, test loss: 1.6571693420410156
Epoch:64, loss=55.357147216796875
Training loss:1.2872756719589233, test loss: 1.6991665363311768
Epoch:65, loss=53.02739334106445
Training loss:1.2330621480941772, test loss: 1.6318838596343994
Epoch:66, loss=52.89254379272461
Training loss:1.179214358329773, test loss: 1.5777101516723633
Epoch:67, loss=49.07292175292969
Training loss:1.3525574207305908, test loss: 1.7619450092315674
Epoch:68, loss=49.454105377197266
Training loss:1.206227421760559, test loss: 1.619713306427002
Epoch:69, loss=56.44845962524414
Training loss:1.259970784187317, test loss: 1.609242558479309
Epoch:70, loss=50.18402099609375
Training loss:1.1544889211654663, test loss: 1.535072684288025
Epoch:71, loss=50.29731750488281
Training loss:1.1781035661697388, test loss: 1.5679899454116821
Epoch:72, loss=48.5538215637207
Training loss:1.5326145887374878, test loss: 1.824927568435669
Epoch:73, loss=48.88084030151367
Training loss:1.850319743156433, test loss: 2.128502130508423
Epoch:74, loss=50.31812286376953
Training loss:1.1657432317733765, test loss: 1.5100910663604736
Epoch:75, loss=48.56998062133789
Training loss:1.149593710899353, test loss: 1.4510513544082642
Epoch:76, loss=47.52546691894531
Training loss:1.1763839721679688, test loss: 1.4968254566192627
Epoch:77, loss=46.37628936767578
Training loss:1.1641716957092285, test loss: 1.5018032789230347
Epoch:78, loss=45.119667053222656
Training loss:1.6824461221694946, test loss: 2.164945602416992
Epoch:79, loss=49.290855407714844
Training loss:1.9894567728042603, test loss: 2.3656728267669678
Epoch:80, loss=52.12744140625
Training loss:1.135812759399414, test loss: 1.4931111335754395
Epoch:81, loss=50.24614715576172
Training loss:1.453258991241455, test loss: 1.79599928855896
Epoch:82, loss=42.26033020019531
Training loss:1.1714754104614258, test loss: 1.5084244012832642
Epoch:83, loss=48.10380172729492
Training loss:2.042123317718506, test loss: 2.4677047729492188
Epoch:84, loss=49.134193420410156
Training loss:4.533116340637207, test loss: 4.946494102478027
Epoch:85, loss=65.83099365234375
Training loss:1.0948035717010498, test loss: 1.4849066734313965
Epoch:86, loss=41.19768142700195
Training loss:0.9717288017272949, test loss: 1.37803316116333
Epoch:87, loss=41.62437438964844
Training loss:1.089056134223938, test loss: 1.4814271926879883
Epoch:88, loss=43.51387023925781
Training loss:0.9889541864395142, test loss: 1.3604307174682617
Epoch:89, loss=40.167903900146484
Training loss:0.940290093421936, test loss: 1.3585257530212402
Epoch:90, loss=38.81828689575195
Training loss:1.015360951423645, test loss: 1.3480125665664673
Epoch:91, loss=39.51177215576172
Training loss:0.9601437449455261, test loss: 1.2994598150253296
Epoch:92, loss=40.7769889831543
Training loss:1.150277018547058, test loss: 1.490354061126709
Epoch:93, loss=40.19805908203125
Training loss:0.9028865098953247, test loss: 1.2879793643951416
Epoch:94, loss=39.22175216674805
Training loss:2.3683526515960693, test loss: 2.6856837272644043
Epoch:95, loss=46.416114807128906
Training loss:1.590193271636963, test loss: 1.972566843032837
Epoch:96, loss=45.450992584228516
Training loss:0.9566327333450317, test loss: 1.3286619186401367
Epoch:97, loss=41.072208404541016
Training loss:1.0129451751708984, test loss: 1.4137928485870361
Epoch:98, loss=39.46583557128906
Training loss:1.9127178192138672, test loss: 2.2950899600982666
Epoch:99, loss=40.51716613769531
Training loss:1.1849960088729858, test loss: 1.5206031799316406
